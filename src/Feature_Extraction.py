import os
import pefile
import csv
import hashlib
import random

# Creating header

MD5_HEADER = ["md5"]

FILE_SIZE = ["FileSize"]

IMAGE_DOS_HEADER = ["e_magic", "e_cblp", "e_cp", "e_crlc", "e_cparhdr", "e_minalloc", "e_maxalloc", "e_ss", "e_sp", \
                    "e_csum", "e_ip", "e_cs", "e_lfarlc", "e_ovno", "e_res", "e_oemid", "e_oeminfo", "e_res2",
                    "e_lfanew"]

FILE_HEADER = ["Machine", "NumberOfSections", "CreationYear", "PointerToSymbolTable", \
               "NumberOfSymbols", "SizeOfOptionalHeader", "Characteristics"]

OPTIONAL_HEADER = ["Magic", "MajorLinkerVersion", "MinorLinkerVersion", "SizeOfCode", "SizeOfInitializedData", \
                   "SizeOfUninitializedData", "AddressOfEntryPoint", \
                   "BaseOfCode", "BaseOfData", "ImageBase", "SectionAlignment", "FileAlignment", \
                   "MajorOperatingSystemVersion", "MinorOperatingSystemVersion", \
                   "MajorImageVersion", \
                   "MinorImageVersion", \
                   "MajorSubsystemVersion", \
                   "MinorSubsystemVersion", \
                   "SizeOfImage", \
                   "SizeOfHeaders", \
                   "CheckSum", \
                   "Subsystem", \
                   "DllCharacteristics", \
                   "SizeOfStackReserve", \
                   "SizeOfStackCommit", \
                   "SizeOfHeapReserve", \
                   "SizeOfHeapCommit", \
                   "LoaderFlags", \
                   "NumberOfRvaAndSizes"]

SECTION_HEADER = ['.rdata', '.data', '.rsrc', '.reloc', '# other sections']
desired_sections = ['.rdata', '.data', '.rsrc', '.reloc']
desired_sections_entropy_virtual_size = ['.idata', '.rdata', '.data', '.rsrc', '.reloc']
SECTION_ENTROPIES_HEADER = ['.idata entropy', '.rdata entropy', '.data entropy', '.rsrc entropy', '.reloc entropy']
SECTION_VIRTUAL_SIZE_HEADER = ['.idata VirtualSize', '.rdata VirtualSize', '.data VirtualSize', '.rsrc VirtualSize', '.reloc VirtualSize']


def md5sum(filename):
    md5 = hashlib.md5()
    with open(filename, 'rb') as f:
        for chunk in iter(lambda: f.read(128 * md5.block_size), b''):
            md5.update(chunk)
    MD5_data = [md5.hexdigest()]
    return MD5_data


def file_creation_year(seconds):
    return 1970 + ((int(seconds) / 86400) / 365)


def get_file_size(filename):
    FILE_SIZE_data = [(os.path.getsize(filename))]
    return FILE_SIZE_data


def extract_dos_header(pe):
    IMAGE_DOS_HEADER_data = [0 for i in range(19)]
    try:
        IMAGE_DOS_HEADER_data = [
            pe.DOS_HEADER.e_magic, \
            pe.DOS_HEADER.e_cblp, \
            pe.DOS_HEADER.e_cp, \
            pe.DOS_HEADER.e_crlc, \
            pe.DOS_HEADER.e_cparhdr, \
            pe.DOS_HEADER.e_minalloc, \
            pe.DOS_HEADER.e_maxalloc, \
            pe.DOS_HEADER.e_ss, \
            pe.DOS_HEADER.e_sp, \
            pe.DOS_HEADER.e_csum, \
            pe.DOS_HEADER.e_ip, \
            pe.DOS_HEADER.e_cs, \
            pe.DOS_HEADER.e_lfarlc, \
            pe.DOS_HEADER.e_ovno, \
            pe.DOS_HEADER.e_res, \
            pe.DOS_HEADER.e_oemid, \
            pe.DOS_HEADER.e_oeminfo, \
            pe.DOS_HEADER.e_res2, \
            pe.DOS_HEADER.e_lfanew]

    except Exception as e:
        print(e)

    return IMAGE_DOS_HEADER_data


def extract_section_header(pe):
    SECTION_HEADER_data = []
    section_names = []

    for section in pe.sections:
        sectionname = ''.join(filter(lambda c: c != '\0', str(section.Name.decode('utf-8'))))
        section_names.append(sectionname)

    for item in desired_sections:
        if item in section_names:
            SECTION_HEADER_data.append("1")
        elif item not in section_names:
            SECTION_HEADER_data.append("0")

    count = 0
    for item in section_names:
        if item not in desired_sections:
            count += 1
        else:
            continue
    SECTION_HEADER_data.append(count)

    return SECTION_HEADER_data


def check_entropy(pe):
    entropies = {}
    PRESENT_ENTROPIES_data = []
    for section in pe.sections:
        sectionname = ''.join(filter(lambda c: c != '\0', str(section.Name.decode('utf-8'))))
        get_entropy = section.get_entropy()
        entropies.update({sectionname: get_entropy})

    for item in desired_sections_entropy_virtual_size :
        for key, value in entropies.items():
            if key == item:
                PRESENT_ENTROPIES_data.append(value)
        if item not in entropies:
            PRESENT_ENTROPIES_data.append(None)
        else:
            continue
    return PRESENT_ENTROPIES_data


def extract_features(pe, filename):

    MD5_data = md5sum(filename)

    FILE_SIZE_data = get_file_size(filename)

    IMAGE_DOS_HEADER_data = extract_dos_header(pe)

    SECTION_HEADER_data = extract_section_header(pe)

    PRESENT_ENTROPIES_data = check_entropy(pe)

    FILE_HEADER_data = [pe.FILE_HEADER.Machine, \
                        pe.FILE_HEADER.NumberOfSections, \
                        file_creation_year(pe.FILE_HEADER.TimeDateStamp), \
                        pe.FILE_HEADER.PointerToSymbolTable, \
                        pe.FILE_HEADER.NumberOfSymbols, \
                        pe.FILE_HEADER.SizeOfOptionalHeader, \
                        pe.FILE_HEADER.Characteristics]

    OPTIONAL_HEADER_data = [pe.OPTIONAL_HEADER.Magic, \
                            pe.OPTIONAL_HEADER.MajorLinkerVersion, \
                            pe.OPTIONAL_HEADER.MinorLinkerVersion, \
                            pe.OPTIONAL_HEADER.SizeOfCode, \
                            pe.OPTIONAL_HEADER.SizeOfInitializedData, \
                            pe.OPTIONAL_HEADER.SizeOfUninitializedData, \
                            pe.OPTIONAL_HEADER.AddressOfEntryPoint, \
                            pe.OPTIONAL_HEADER.BaseOfCode, \
                            pe.OPTIONAL_HEADER.BaseOfData, \
                            pe.OPTIONAL_HEADER.ImageBase, \
                            pe.OPTIONAL_HEADER.SectionAlignment, \
                            pe.OPTIONAL_HEADER.FileAlignment, \
                            pe.OPTIONAL_HEADER.MajorOperatingSystemVersion, \
                            pe.OPTIONAL_HEADER.MinorOperatingSystemVersion, \
                            pe.OPTIONAL_HEADER.MajorImageVersion, \
                            pe.OPTIONAL_HEADER.MinorImageVersion, \
                            pe.OPTIONAL_HEADER.MajorSubsystemVersion, \
                            pe.OPTIONAL_HEADER.MinorSubsystemVersion, \
                            pe.OPTIONAL_HEADER.SizeOfImage, \
                            pe.OPTIONAL_HEADER.SizeOfHeaders, \
                            pe.OPTIONAL_HEADER.CheckSum, \
                            pe.OPTIONAL_HEADER.Subsystem, \
                            pe.OPTIONAL_HEADER.DllCharacteristics, \
                            pe.OPTIONAL_HEADER.SizeOfStackReserve, \
                            pe.OPTIONAL_HEADER.SizeOfStackCommit, \
                            pe.OPTIONAL_HEADER.SizeOfHeapReserve, \
                            pe.OPTIONAL_HEADER.SizeOfHeapCommit, \
                            pe.OPTIONAL_HEADER.LoaderFlags, \
                            pe.OPTIONAL_HEADER.NumberOfRvaAndSizes]

    return MD5_data + FILE_SIZE_data + IMAGE_DOS_HEADER_data + FILE_HEADER_data + OPTIONAL_HEADER_data + SECTION_HEADER_data + PRESENT_ENTROPIES_data


# Set directories
rootdir = "/media/sandeep/DATA/raw_final_dataset/RAW_final_dataset/"
output = "/media/sandeep/DATA/raw_final_dataset/Various_samples_header_features.csv"
subdirectories = {"unclassified": [0], "Adware": [1], "Backdoor": [2], "Generic": [3], "Trojan": [4], "Virus": [5], "Worm": [6]}
subset_size = 2000

# Prepare csv file
f = open(output, mode="w", encoding='utf-8')
writer = csv.writer(f, delimiter=',')
writer.writerow(MD5_HEADER + FILE_SIZE + IMAGE_DOS_HEADER + FILE_HEADER + OPTIONAL_HEADER + SECTION_HEADER + SECTION_ENTROPIES_HEADER + ['class'])

# Extract features and write to csv
for subdirectory, class_label in subdirectories.items():
    new_directory = os.path.join(rootdir, subdirectory)
    for root, dirs, files in os.walk(new_directory):
        class_samples = random.sample(files, subset_size)
        for file in class_samples:
            input_file = new_directory + '/' + file
            try:
                pe = pefile.PE(input_file)
            except Exception as e:
                print("Exception while loading file: ", e)
            else:
                try:
                    features = extract_features(pe, input_file)
                    writer.writerow(features + class_label)
                except Exception as e:
                    print("Exception while opening and writing CSV file: ", e)

f.close()
