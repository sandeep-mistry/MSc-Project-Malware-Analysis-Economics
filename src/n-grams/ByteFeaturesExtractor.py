import numpy as np
from sklearn.feature_extraction import DictVectorizer
from common import *
from Grebber import chunks
from multiprocessing import Process
from scipy.stats import entropy
import Reader as r
import pickle
from progressbar import Percentage, Bar, ProgressBar, ETA
import sys


def byteEntropy(byteFreq):
    probs = np.array(byteFreq)
    probs = probs / probs.sum()
    ent = entropy(probs)
    return ent


def findIndex(lst):
    res = 0
    for idx, val in enumerate(lst):
        res += val * (257 ** idx)
    return res


def byteSeqToInt(seq):
    bytes = []
    for row in seq:
        for item in row.split(' ')[1:]:
            if item != "??":
                bytes.append(int(item, 16))
            else:
                bytes.append(256)
    return bytes


def processFile(text):
    bytes = byteSeqToInt(text)

    # 1gram
    byte1gram = [0] * 257
    for item in bytes:
        byte1gram[item] += 1

    # 2gram
    # n = 2
    # byte2gram = {}
    # for i in range(len(bytes)-n+1):
    # idx = findIndex(bytes[i:i+n])
    # byte2gram[idx] = byte2gram.get(idx, 0) + 1

    # 4gram
    # n = 4
    # byte4gram = {}
    # for i in range(len(bytes)-n+1):
    # idx = findIndex(bytes[i:i+n])
    # byte4gram[idx] = byte4gram.get(idx, 0) + 1

    res = {'1g': byte1gram, '1f': byteEntropy(byte1gram)}

    return res


def worker(chunk, output_dir):
    widgets = ['Worker {}: '.format(chunk[0]), ' ', Percentage(), ' ', Bar(), ' ', ETA()]
    pbar = ProgressBar(widgets=widgets, maxval=len(chunk[1])).start()
    for idx, item in enumerate(chunk[1]):
        fn = item.split(os.sep)[-1].split(".")[0]

        # Save data
        data = processFile(r.readBytes(item))
        pickle.dump(data, open(os.path.join(output_dir, fn), "wb"))

        # Update progressbar
        pbar.update(idx + 1)
    pbar.finish()


if __name__ == "__main__":
    print("Byte feature extraction started!")

    if sys.argv[1] == "train":
        main_path = os.path.join(root_path, "train")
        split_data = chunks(train_set, numWorkers)
    else:
        main_path = os.path.join(root_path, "test")
        split_data = chunks(test_set, numWorkers)

    output_dir = os.path.join(main_path, byte_feat_dir)

    clearPath(output_dir)

    jobs = []
    for item in enumerate(split_data):
        p = Process(target=worker, args=(item, output_dir))
        jobs.append(p)
        p.start()

    for j in jobs:
        j.join()

    print("Byte feature extraction completed!")
