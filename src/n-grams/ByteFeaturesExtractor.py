
import numpy as np
from sklearn.feature_extraction import DictVectorizer
from common import *
from Grebber import chunks
from multiprocessing import Process
from scipy.stats import entropy
import Reader as r
import pickle
from progressbar import Percentage, Bar, ProgressBar, ETA
import sys


def byteEntropy(byteFreq):
    probs = np.array(byteFreq)
    probs = probs/probs.sum()
    ent = entropy(probs)
    return ent
    
    
def findIndex(lst):
    res = 0
    for idx, val in enumerate(lst):
        res += val*(257**idx)
    return res
    
    
def byteSeqToInt(seq):
    bytes = []
    for row in seq:
        for item in row.split(' ')[1:]:
            if item != "??":
                bytes.append(int(item, 16))
            else:
                bytes.append(256)
    return bytes
    
    
def processFile(text):
    bytes = byteSeqToInt(text)
    
    # 1gram
    byte1gram = [0]*257
    for item in bytes:
        byte1gram[item] += 1
        
    # 2gram
    # n = 2
    # byte2gram = {}
    # for i in range(len(bytes)-n+1):
        # idx = findIndex(bytes[i:i+n])
        # byte2gram[idx] = byte2gram.get(idx, 0) + 1
    
    # 4gram
    # n = 4
    # byte4gram = {}
    # for i in range(len(bytes)-n+1):
        # idx = findIndex(bytes[i:i+n])
        # byte4gram[idx] = byte4gram.get(idx, 0) + 1
        
    res = {'1g': byte1gram, '1f':byteEntropy(byte1gram)}
    
    return res
    
        
def worker(chunk, output_dir):
    widgets = ['Worker {}: '.format(chunk[0]), ' ', Percentage(), ' ', Bar(), ' ', ETA()]
    pbar = ProgressBar(widgets=widgets, maxval=len(chunk[1])).start()
    for idx, item in enumerate(chunk[1]):
        fn = item.split(os.sep)[-1].split(".")[0]
        
        # Save data
        data = processFile(r.readBytes(item))
        pickle.dump(data, open(os.path.join(output_dir, fn), "wb"))
        
        # Update progressbar
        pbar.update(idx+1)
    pbar.finish()
                     

if __name__ == "__main__":
    print("Byte feature extraction started!")
    
    if sys.argv[1] == "train":
        main_path = os.path.join(root_path, "train")
        split_data = chunks(train_bytes, numWorkers)
    else:
        main_path = os.path.join(root_path, "test")
        split_data = chunks(test_bytes, numWorkers)
    
    output_dir = os.path.join(main_path, byte_feat_dir)
    
    clearPath(output_dir)
        
    jobs = []        
    for item in enumerate(split_data):
        p = Process(target=worker, args=(item, output_dir))
        jobs.append(p)
        p.start()
        
    for j in jobs:
        j.join()
    
    print("Byte feature extraction completed!")
        
