# XGBoost model training

from common import *
import numpy as np
import pickle
import xgboost
import sys
import scipy

from sklearn.feature_selection import SelectKBest, f_regression, SelectFromModel, chi2
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score
from sklearn.preprocessing import normalize

# Cross validation training
# Returns history of logloss errors
def trainCV(trainMatrix, train_label):
    params = {
              'learning_rate': 0.05,
              'subsample': 0.5,
              'max_depth': 7,
              'colsample_bytree': 0.5,
              'min_child_weight': 0.05,
              'silent':1,
              'gamma':0.001,
              'objective':'multi:softprob',
              'num_class':9}
    numRounds = 1200
    modelName = "model1"
    dm_train = xgboost.DMatrix(trainMatrix, label=train_label)
    evalHist = xgboost.cv(params, dm_train, numRounds, nfold=5, metrics=['mlogloss', 'merror'], verbose_eval=True,seed=0)
    pickle.dump(evalHist, open(os.path.join(root_path, modelName+"_res"), "wb"))
    return evalHist

# Training without CV
# Returns the classifier created
def train(trainMatrix, train_label):
    params = {
              'learning_rate': 0.1,
              'eta':0.1,
              'subsample': 0.8,
              'max_depth': 3,
              'colsample_bytree': 0.7,
              'min_child_weight': 5,
              'silent':1,
              'gamma':0.01,
              'objective':'multi:softprob',
              'eval_metric':'mlogloss',
              'num_class':9}
    numRounds = 150
    dMatrix = xgboost.DMatrix(trainMatrix, label=train_label)
    classifier = xgboost.train(params, dMatrix, numRounds)
    return classifier
