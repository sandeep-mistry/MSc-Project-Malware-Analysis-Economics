import random
import os
import shutil
import Reader as r
import pandas as pd
import pickle
from operator import itemgetter
from collections import OrderedDict
import itertools
import numpy as np


# Delete path if exists
def deleteIfExists(filename):
    if os.path.exists(filename):
        os.remove(filename)


# Delete directory and create a new one  
def clearPath(dirPath):
    if os.path.exists(dirPath):
        shutil.rmtree(dirPath)
    os.makedirs(dirPath)


# Export extraction times to .dat file
def extraction_time_export(filename, extraction_time):
    extraction_times = []
    output_filename = filename
    if os.path.exists(output_filename):
        with open(output_filename, 'rb') as rfp:
            extraction_times = pickle.load(rfp)
    extraction_times.append(extraction_time)
    with open(output_filename, 'wb') as wfp:
        pickle.dump(extraction_times, wfp)


def get_top_n_grams_features(feat_files, gram):
    array = []
    for file in feat_files:
        res = [pickle.load(open(file, "rb"))]
        byte3g_dicts = [item[gram] for item in res]
        for entry in byte3g_dicts:
            new_dict = entry

        sorted_x = dict(OrderedDict(sorted(new_dict.items(), key=itemgetter(1), reverse=True)))
        byte3g_dicts = [dict(itertools.islice(sorted_x.items(), 500))]
        array.append(byte3g_dicts)

    return np.array(array)


# Global paths
root_path = "/home/sandeep/outputs/"
# train_dir = "/home/sandeep/hexdumps/all-hexdumps/"
train_dir = "/mnt/disks/data/hexdumps/all-hexdumps/"
# train_dir = "/media/sandeep/DATA/raw_final_dataset/Hexdumps/train_set/"
numWorkers = 8
# subset_size = 100

dataset = [os.path.join(train_dir, f) for f in os.listdir(train_dir) if
             os.path.isfile(os.path.join(train_dir, f))]

# train_set = dataset

csv_path = "/home/sandeep/Labels.csv"
label_data = pd.read_csv(csv_path)
labels = {row['MD5hash']: row['ClassLabel'] for index, row in label_data.iterrows()}

train_label = [labels[i.split(os.sep)[-1].split(".")[0]] - 1 for i in dataset]

model_dir = os.path.join(root_path, "model")
feat_dir = os.path.join(root_path, "feat")
time_dir_bytes = os.path.join(root_path, 'extraction_times_bytes')
time_dir_feature_matrices = os.path.join(root_path, 'extraction_times_feature_matrices')

# Byte paths
byte_feat_dir = os.path.join(feat_dir, "byte")
byte1g_matrix_path = os.path.join(model_dir, "byte1g_matrix.npy")
byte1f_matrix_path = os.path.join(model_dir, "byte1f_matrix.npy")
byte2g_matrix_path = os.path.join(model_dir, "byte2g_matrix.npy")
byte3g_matrix_path = os.path.join(model_dir, "byte3g_matrix.npz")
byte4g_matrix_path = os.path.join(model_dir, "byte4g_matrix.npz")

# Important features paths
byte2g_imp_feature_path = os.path.join(model_dir, "byte2g_imp_feature")
byte3g_imp_feature_path = os.path.join(model_dir, "byte3g_imp_feature")
byte1g_imp_feature_path = os.path.join(model_dir, "byte1g_imp_feature")

final_classifier_path = os.path.join(root_path, "final_classifier")

# Selected features paths
selected_features_path = os.path.join(root_path, "selected_features")
