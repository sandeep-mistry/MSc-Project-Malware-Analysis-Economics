from common import *
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import normalize
import numpy as np
import pickle
import sys
import scipy.sparse
import scipy
import time
from sklearn import decomposition
from sklearn.decomposition import TruncatedSVD
import random

main_path = root_path

# Boring path names
model_dir = os.path.join(main_path, model_dir)
byte_feat_dir = os.path.join(main_path, byte_feat_dir)
byte1g_matrix_path = os.path.join(main_path, byte1g_matrix_path)
byte1f_matrix_path = os.path.join(main_path, byte1f_matrix_path)
byte2g_matrix_path = os.path.join(main_path, byte2g_matrix_path)
byte3g_matrix_path = os.path.join(main_path, byte3g_matrix_path)
byte4g_matrix_path = os.path.join(main_path, byte4g_matrix_path)

# There is a bug of pickle for overwriting previous files if newer file is smaller
# So just clear the folder!
# clearPath(model_dir)

#############################   BYTE FEATURES   ###########################################################
print("Byte features started")
# Load features
dataset_sample = random.sample(dataset, 100)
feat_files = [os.path.join(byte_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in dataset_sample]
res = [pickle.load(open(file, "rb"), encoding='latin1') for file in feat_files]

# # 1gram entropy
# byte1f_matrix = np.array([pickle.load(open(file, "rb"))['1f'] for file in feat_files])
# np.save(byte1f_matrix_path, byte1f_matrix)
# del byte1f_matrix
# byte1g_matrix = np.array([pickle.load(open(file, "rb"))['1g'] for file in feat_files])
# byte1g_matrix = normalize(byte1g_matrix, axis=1, norm='l1')
# np.save(byte1g_matrix_path, byte1g_matrix)
# del byte1g_matrix

start_time = time.time()

if __name__ == "__main__":

    if sys.argv[1] == "1gram":
        # 1gram
        byte1g_matrix = np.array([pickle.load(open(file, "rb"), encoding='latin1')['1g'] for file in feat_files])
        byte1g_matrix = normalize(byte1g_matrix, axis=1, norm='l1')
        np.save(byte1g_matrix_path, byte1g_matrix)
        extraction_time_1g = time.time() - start_time
        extraction_time_export('1_gram_feature_matrix.dat', extraction_time_1g)
        del byte1g_matrix

    elif sys.argv[1] == "2gram":
        # 2gram
        # byte2g_dicts = np.array([pickle.load(open(file, "rb"), encoding='latin1')['2g'] for file in feat_files])
        byte2g_dicts = [item['2g'] for item in res]
        del res
        dictVectorizer = DictVectorizer(sparse=True)
        byte2g_matrix = dictVectorizer.fit_transform(byte2g_dicts).tocsr()
        scipy.sparse.save_npz(byte2g_matrix_path, byte2g_matrix)
        # byte2g_matrix = normalize(byte2g_matrix, axis=1, norm='l1')
        # byte2g_matrix = byte2g_matrix.toarray()
        feat_names = dictVectorizer.get_feature_names()
        # np.save(byte2g_matrix_path, byte2g_matrix)
        extraction_time_2g = time.time() - start_time
        extraction_time_export('2_gram_feature_matrix.dat', extraction_time_2g)
        del byte2g_matrix
        del byte2g_dicts

    elif sys.argv[1] == "3gram":
        # 3gram
        byte3g_dicts = [item['3g'] for item in res]
        del res
        dictVectorizer = DictVectorizer(sparse=True)
        byte3g_matrix = dictVectorizer.fit_transform(byte3g_dicts).tocsr()
        svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)
        byte3g_matrix = svd.fit(byte3g_matrix)
        scipy.sparse.save_npz(byte3g_matrix_path, byte3g_matrix)
        # feat_names = dictVectorizer.get_feature_names()
        extraction_time_3g = time.time() - start_time
        extraction_time_export('3_gram_feature_matrix.dat', extraction_time_3g)
        del byte3g_matrix
        del byte3g_dicts

    else:
        exit()
