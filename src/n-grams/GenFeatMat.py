
from common import *
from sklearn.feature_extraction import DictVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import normalize
import numpy as np
import pickle
import sys


if __name__ == "__main__":

    # Which dataset? Training or testing?
    if sys.argv[1] == "train":
        main_path = os.path.join(root_path, "train")
        t = train_asm
    else:
        main_path = os.path.join(root_path, "test")
        t = test_asm

    # Boring path names
    model_dir = os.path.join(main_path, model_dir)
    dll_feat_dir = os.path.join(main_path, dll_feat_dir)
    dll_matrix_path = os.path.join(main_path, dll_matrix_path)
    opcode_feat_dir = os.path.join(main_path, opcode_feat_dir)
    opcode_matrix_path = os.path.join(main_path, opcode_matrix_path)
    sectionhist_feat_dir = os.path.join(main_path, sectionhist_feat_dir)
    section_matrix_path = os.path.join(main_path, section_matrix_path)
    stdcall_feat_dir = os.path.join(main_path, stdcall_feat_dir)
    stdcall_matrix_path = os.path.join(main_path, stdcall_matrix_path)
    byte_feat_dir = os.path.join(main_path, byte_feat_dir)
    byte1g_matrix_path = os.path.join(main_path, byte1g_matrix_path)
    byte1f_matrix_path = os.path.join(main_path, byte1f_matrix_path)
    byte2g_matrix_path = os.path.join(main_path, byte2g_matrix_path)
    byte4g_matrix_path = os.path.join(main_path, byte4g_matrix_path)
    byte_image_feat_dir = os.path.join(main_path, byte_image_feat_dir)
    byte_image_matrix_path = os.path.join(main_path, byte_image_matrix_path)
    length_matrix_path = os.path.join(main_path, length_matrix_dir)
    length_feat_dir = os.path.join(main_path, length_feat_dir)

    # There is a bug of pickle for overwriting previous files if newer file is smaller
    # So just clear the folder!
    clearPath(model_dir)

    ##############################      DLL        #############################################################
    print("DLL features started")
    # Load features
    feat_files = [os.path.join(dll_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in t]
    res = [pickle.load(open(file, "rb")) for file in feat_files]

    # DictVectorizer
    dictVectorizer = DictVectorizer(sparse=False)
    dll_matrix = dictVectorizer.fit_transform(res)
    # dll_matrix = normalize(dll_matrix, axis=1, norm='l1')
    np.save(dll_matrix_path, dll_matrix)
    del dll_matrix, dictVectorizer

    #############################     OPCODE       ############################################################
    print("Opcode features started")
    # Load features
    feat_files = [os.path.join(opcode_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in t]
    res = [pickle.load(open(file, "rb")) for file in feat_files]

    # DictVectorizer
    dictVectorizer = DictVectorizer(sparse=False)
    opcode_matrix = dictVectorizer.fit_transform(res)
    # opcode_matrix = normalize(opcode_matrix, axis=1, norm='l1')
    feat_names = dictVectorizer.get_feature_names()
    # pickle.dump(feat_names, open(opcode_feat_names, "wb"))
    np.save(opcode_matrix_path, opcode_matrix)
    del opcode_matrix, dictVectorizer

    #############################     SECTION LENGTH      #####################################################
    print("Sectionhist features started")
    # Load features
    feat_files = [os.path.join(sectionhist_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in t]
    res = [pickle.load(open(file, "rb")) for file in feat_files]

    # DictVectorizer
    dictVectorizer = DictVectorizer(sparse=False)
    section_matrix = dictVectorizer.fit_transform(res)
    # section_matrix = normalize(section_matrix, axis=1, norm='l1')
    feat_names = dictVectorizer.get_feature_names()
    # pickle.dump(feat_names, open(section_feat_names, "wb"))
    np.save(section_matrix_path, section_matrix)
    del section_matrix, dictVectorizer

    ##############################    STDCALL  #################################################################
    print("Stdcall features started")
    # Load features
    feat_files = [os.path.join(stdcall_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in t]
    # Choose functions that appear in at least 5 different files. These are in cut_list list.
    cut_list = pickle.load(open(stdcall_cut_path, "rb"))
    res = []

    for file in feat_files:
        d = pickle.load(open(file, "rb"))
        newDict = {key:0 for key in cut_list.items()}
        for key,value in d.items():
            if key in newDict:
                newDict[key] = value
        res.append(newDict)

    # DictVectorizer
    dictVectorizer = DictVectorizer(sparse=False)
    stdcall_matrix = dictVectorizer.fit_transform(res)
    # stdcall_matrix = normalize(stdcall_matrix, axis=1, norm='l1')
    feat_names = dictVectorizer.get_feature_names()
    # pickle.dump(feat_names, open(stdcall_feat_names, "wb"))
    np.save(stdcall_matrix_path, stdcall_matrix)
    del stdcall_matrix, dictVectorizer

    #############################   BYTE FEATURES   ###########################################################
    print("Byte features started")
    # Load features
    feat_files = [os.path.join(byte_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in t]
    # res = [pickle.load(open(file, "rb")) for file in feat_files]

    # 1gram
    byte1g_matrix = np.array([pickle.load(open(file, "rb"))['1g'] for file in feat_files])
    byte1g_matrix = normalize(byte1g_matrix, axis=1, norm='l1')
    np.save(byte1g_matrix_path, byte1g_matrix)
    del byte1g_matrix

    # 1gram entropy
    byte1f_matrix = np.array([pickle.load(open(file, "rb"))['1f'] for file in feat_files])
    np.save(byte1f_matrix_path, byte1f_matrix)
    del byte1f_matrix

    # 2gram
    # byte2g_dicts = np.array([pickle.load(open(file, "rb"))['2g'] for file in feat_files])
    # dictVectorizer = DictVectorizer()
    # byte2g_matrix = dictVectorizer.fit_transform(byte2g_dicts)
    # byte2g_matrix = normalize(byte2g_matrix, axis=1, norm='l1')
    # feat_names = dictVectorizer.get_feature_names()
    # np.save(byte2g_matrix_path, byte2g_matrix)
    # del byte2g_matrix


    # 4gram
    # byte4g_dicts = [item['4g'] for item in res]
    # del res
    # dictVectorizer = DictVectorizer(sparse=True)
    # byte4g_matrix = dictVectorizer.fit_transform(byte4g_dicts)
    # feat_names = dictVectorizer.get_feature_names()
    # np.save(byte4g_matrix_path, byte4g_matrix)
    # del byte4g_matrix
    # del byte4g_dicts


    ##############################    BYTE IMAGE    ###################################################
    print("Byte Image started")
    # Load features
    feat_files = [os.path.join(byte_image_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in t]
    res = [pickle.load(open(file, "rb")) for file in feat_files]

    byte_image_matrix = np.asarray(res, dtype=np.float64)
    np.save(byte_image_matrix_path, byte_image_matrix)

   ###################    FILE LENGTH      ####################################
    print("File length features started")
    # Load features
    feat_files = [os.path.join(length_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in t]
    res = [pickle.load(open(file, "rb")) for file in feat_files]

    length_matrix = np.asarray(res, dtype=np.float64)
    np.save(length_matrix_path, length_matrix)
    del length_matrix
