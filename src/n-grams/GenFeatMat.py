from common import *
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import normalize
import numpy as np
import pickle
import sys
import scipy.sparse
import scipy
import time
from sklearn import decomposition
from sklearn.decomposition import TruncatedSVD
import random
from sklearn.decomposition import PCA
from scipy.sparse import *
from operator import itemgetter
from collections import OrderedDict
import itertools

main_path = root_path

# Boring path names
model_dir = os.path.join(main_path, model_dir)
byte_feat_dir = os.path.join(main_path, byte_feat_dir)
byte1g_matrix_path = os.path.join(main_path, byte1g_matrix_path)
byte1f_matrix_path = os.path.join(main_path, byte1f_matrix_path)
byte2g_matrix_path = os.path.join(main_path, byte2g_matrix_path)
byte3g_matrix_path = os.path.join(main_path, byte3g_matrix_path)
byte4g_matrix_path = os.path.join(main_path, byte4g_matrix_path)

# There is a bug of pickle for overwriting previous files if newer file is smaller
# So just clear the folder!
# clearPath(model_dir)

#############################   BYTE FEATURES   ###########################################################
print("Byte features started")
# Load features
# dataset_sample = random.sample(dataset, 100)
feat_files = [os.path.join(byte_feat_dir, file.split(os.sep)[-1].split(".")[0]) for file in dataset]
# res = [pickle.load(open(file, "rb")) for file in feat_files]

array = []
for file in feat_files:
    res = [pickle.load(open(file, "rb"))]
    byte3g_dicts = [item['3g'] for item in res]
    for entry in byte3g_dicts:
        new_dict = entry

    sorted_x = dict(OrderedDict(sorted(new_dict.items(), key=itemgetter(1), reverse=True)))
    byte3g_dicts = [dict(itertools.islice(sorted_x.items(), 500))]
    array.append(byte3g_dicts)

byte3g_dicts = np.array(array)


print("Loaded res")

# # 1gram entropy
# byte1f_matrix = np.array([pickle.load(open(file, "rb"))['1f'] for file in feat_files])
# np.save(byte1f_matrix_path, byte1f_matrix)
# del byte1f_matrix
# byte1g_matrix = np.array([pickle.load(open(file, "rb"))['1g'] for file in feat_files])
# byte1g_matrix = normalize(byte1g_matrix, axis=1, norm='l1')
# np.save(byte1g_matrix_path, byte1g_matrix)
# del byte1g_matrix

start_time = time.time()

if __name__ == "__main__":

    if sys.argv[1] == "1gram":
        # 1gram
        byte1g_matrix = np.array([pickle.load(open(file, "rb"))['1g'] for file in feat_files])
        byte1g_matrix = normalize(byte1g_matrix, axis=1, norm='l1')
        np.save(byte1g_matrix_path, byte1g_matrix)
        extraction_time_1g = time.time() - start_time
        extraction_time_export('1_gram_feature_matrix.dat', extraction_time_1g)
        del byte1g_matrix

    elif sys.argv[1] == "2gram":
        # 2gram
        # byte2g_dicts = np.array([pickle.load(open(file, "rb"))['2g'] for file in feat_files])
        # byte2g_dicts = [item['2g'] for item in res]
        byte2g_dicts = get_top_n_grams_features(feat_files, '2g')
        print(("Loaded dicts"))
        # del res
        dictVectorizer = DictVectorizer(sparse=False)
        byte2g_matrix = dictVectorizer.fit_transform(byte2g_dicts)
        # scipy.sparse.save_npz(byte2g_matrix_path, byte2g_matrix)
        # byte2g_matrix = normalize(byte2g_matrix, axis=1, norm='l1')
        # byte2g_matrix = byte2g_matrix.toarray()
        feat_names = dictVectorizer.get_feature_names()
        np.save(byte2g_matrix_path, byte2g_matrix)
        extraction_time_2g = time.time() - start_time
        extraction_time_export('2_gram_feature_matrix.dat', extraction_time_2g)
        del byte2g_matrix
        del byte2g_dicts

    elif sys.argv[1] == "3gram":
        # 3gram
        byte3g_dicts = [item['3g'] for item in res]
        del res
        dictVectorizer = DictVectorizer(sparse=True)
        byte3g_matrix = dictVectorizer.fit_transform(byte3g_dicts).tocsr()
        svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)
        # pca = PCA(n_components=100, svd_solver='full')
        byte3g_matrix = svd.fit(byte3g_matrix)
        # byte3g_matrix = pca.fit(byte3g_matrix)
        scipy.sparse.save_npz(byte3g_matrix_path, byte3g_matrix)
        # feat_names = dictVectorizer.get_feature_names()
        extraction_time_3g = time.time() - start_time
        extraction_time_export('3_gram_feature_matrix.dat', extraction_time_3g)
        del byte3g_matrix
        del byte3g_dicts

    else:
        exit()
