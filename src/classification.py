import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_selection import SelectFromModel
import sklearn.ensemble as ske
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score
import csv


# Importing the dataset
csv_path = "/home/sandeep/outputs/header_features.csv"
dataset = pd.read_csv(csv_path)
X = dataset.drop(['md5', 'e_res', 'e_res2', 'class'], axis=1).values
for cols in X.columns.tolist()[1:]:
    X = X.ix[X[cols] > 0]
y = dataset['class'].values

# Substitute any NaN values with column mean
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp = imp.fit(X)
X = imp.transform(X)

fsel = ske.GradientBoostingClassifier().fit(X, y)
model = SelectFromModel(fsel, prefit=True)
X_new = model.transform(X)
nb_features = X_new.shape[1]
indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]
with open('top_features.txt', 'w') as file:
    for f in range(nb_features):
        print("%d. feature %s (%f)" % (f + 1, dataset.columns[2 + indices[f]], fsel.feature_importances_[indices[f]]))
        file.write(("%d. feature %s (%f)\n" % (f + 1, dataset.columns[2 + indices[f]], fsel.feature_importances_[indices[f]])))
features = []
for f in sorted(np.argsort(fsel.feature_importances_)[::-1][:nb_features]):
    features.append(dataset.columns[2 + f])


def roc_auc_score_multiclass(actual_class, pred_class, average="macro"):
    # creating a set of all the unique classes using the actual class list
    unique_class = set(actual_class)
    roc_auc_dict = {}
    for per_class in unique_class:
        # creating a list of all the classes except the current class
        other_class = [x for x in unique_class if x != per_class]

        # marking the current class as 1 and all other classes as 0
        new_actual_class = [0 if x in other_class else 1 for x in actual_class]
        new_pred_class = [0 if x in other_class else 1 for x in pred_class]

        # using the sklearn metrics method to calculate the roc_auc_score
        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average=average)
        roc_auc_dict[per_class] = roc_auc

    return roc_auc_dict


def classify(training_matrix, classifier):
    X_train, X_test, y_train, y_test = train_test_split(training_matrix, y, test_size=0.2, random_state=7)
    kfold = KFold(n_splits=10, random_state=7)
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    classifier.fit(X_train, y_train)
    # predict the test results
    cross_val_results = cross_val_score(classifier, X_train, y_train, cv=kfold) * 100
    cross_val_mean = np.mean(cross_val_results)
    cross_val_variance = np.var(cross_val_results)
    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred) * 100
    # confusion_mat = confusion_matrix(y_test, y_pred)
    auc_score = roc_auc_score_multiclass(y_test, y_pred)
    return [accuracy, auc_score, cross_val_results, cross_val_mean, cross_val_variance]


# classifier_names = ["Random Forest", "Naive Bayes", "XGBoost", "KNN", "MLP"]
classifier_names = ["Random Forest", "XGBoost", "KNN", "MLP"]

# classifiers = [
#     RandomForestClassifier(n_estimators=50, criterion='entropy', random_state=7),
#     MultinomialNB(),
#     XGBClassifier(max_depth=20, learning_rate=0.3, n_estimators=150),
#     KNeighborsClassifier(5),
#     MLPClassifier(alpha=1, max_iter=2000)]
classifiers = [
    RandomForestClassifier(n_estimators=50, criterion='entropy', random_state=7),
    XGBClassifier(max_depth=20, learning_rate=0.3, n_estimators=150),
    KNeighborsClassifier(5),
    MLPClassifier(alpha=1, max_iter=2000)]

# Prepare csv file
output_csv = "/home/sandeep/outputs/header_classifier_accuracies.csv"


def multiple_classifications(clf_names, clfs):
    f = open(output_csv, mode="w", encoding='utf-8')
    writer = csv.writer(f, delimiter=',')
    writer.writerow(['Classifier'] + ['Accuracy'] + ['AUC Score'] + ['Cross Val Score'] + ['Cross Val Mean'] + \
                    ['Cross Val Variance'])

    for name, clf in zip(clf_names, clfs):
        classification_scores = classify(X_new, clf)
        writer.writerow([name] + [str(classification_scores[0])] + [str(classification_scores[1])] + \
                                [str(classification_scores[2])] + [str(classification_scores[3])] + \
                                [str(classification_scores[4])])
    f.close()


multiple_classifications(classifier_names, classifiers)
